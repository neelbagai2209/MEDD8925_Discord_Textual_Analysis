{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c9c70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries (deduplicated): 172581\n",
      "Categories exported to all_wordnet_categories.csv\n",
      "                word    category subcategory  \\\n",
      "0           ABDUCTED  ADJECTIVES     ADJ.PPL   \n",
      "1          ADSORBING  ADJECTIVES     ADJ.PPL   \n",
      "2            AVENGED  ADJECTIVES     ADJ.PPL   \n",
      "3         CALIBRATED  ADJECTIVES     ADJ.PPL   \n",
      "4          CANTERING  ADJECTIVES     ADJ.PPL   \n",
      "5         CARBONIZED  ADJECTIVES     ADJ.PPL   \n",
      "6            CHARRED  ADJECTIVES     ADJ.PPL   \n",
      "7   CLOSED-CAPTIONED  ADJECTIVES     ADJ.PPL   \n",
      "8      CONTAINERISED  ADJECTIVES     ADJ.PPL   \n",
      "9      CONTAINERIZED  ADJECTIVES     ADJ.PPL   \n",
      "10         CONTESTED  ADJECTIVES     ADJ.PPL   \n",
      "11            COOING  ADJECTIVES     ADJ.PPL   \n",
      "12         CORBELLED  ADJECTIVES     ADJ.PPL   \n",
      "13  COUNTERBALANCING  ADJECTIVES     ADJ.PPL   \n",
      "14          CRUNCHED  ADJECTIVES     ADJ.PPL   \n",
      "15           CURSING  ADJECTIVES     ADJ.PPL   \n",
      "16           DEPOSED  ADJECTIVES     ADJ.PPL   \n",
      "17           DIMPLED  ADJECTIVES     ADJ.PPL   \n",
      "18            DUMPED  ADJECTIVES     ADJ.PPL   \n",
      "19           ELAPSED  ADJECTIVES     ADJ.PPL   \n",
      "20          EMITTING  ADJECTIVES     ADJ.PPL   \n",
      "21      EXTRAPOLATED  ADJECTIVES     ADJ.PPL   \n",
      "22         GALLOPING  ADJECTIVES     ADJ.PPL   \n",
      "23          HAMMERED  ADJECTIVES     ADJ.PPL   \n",
      "24          HANDHELD  ADJECTIVES     ADJ.PPL   \n",
      "\n",
      "                                  source_file  \n",
      "0   WordNet Words & Phrases (Unambiguous).txt  \n",
      "1   WordNet Words & Phrases (Unambiguous).txt  \n",
      "2   WordNet Words & Phrases (Unambiguous).txt  \n",
      "3   WordNet Words & Phrases (Unambiguous).txt  \n",
      "4   WordNet Words & Phrases (Unambiguous).txt  \n",
      "5   WordNet Words & Phrases (Unambiguous).txt  \n",
      "6   WordNet Words & Phrases (Unambiguous).txt  \n",
      "7   WordNet Words & Phrases (Unambiguous).txt  \n",
      "8   WordNet Words & Phrases (Unambiguous).txt  \n",
      "9   WordNet Words & Phrases (Unambiguous).txt  \n",
      "10  WordNet Words & Phrases (Unambiguous).txt  \n",
      "11  WordNet Words & Phrases (Unambiguous).txt  \n",
      "12  WordNet Words & Phrases (Unambiguous).txt  \n",
      "13  WordNet Words & Phrases (Unambiguous).txt  \n",
      "14  WordNet Words & Phrases (Unambiguous).txt  \n",
      "15  WordNet Words & Phrases (Unambiguous).txt  \n",
      "16  WordNet Words & Phrases (Unambiguous).txt  \n",
      "17  WordNet Words & Phrases (Unambiguous).txt  \n",
      "18  WordNet Words & Phrases (Unambiguous).txt  \n",
      "19  WordNet Words & Phrases (Unambiguous).txt  \n",
      "20  WordNet Words & Phrases (Unambiguous).txt  \n",
      "21  WordNet Words & Phrases (Unambiguous).txt  \n",
      "22  WordNet Words & Phrases (Unambiguous).txt  \n",
      "23  WordNet Words & Phrases (Unambiguous).txt  \n",
      "24  WordNet Words & Phrases (Unambiguous).txt  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the txt files\n",
    "txt_dir = r\"C:\\Users\\neelb\\Desktop\\MEDD8925 - Analyzing Data Quantitatively\\MEDD8925 Discord Project\\WordNet\"\n",
    "\n",
    "# Find all relevant .txt files except LICENSE.txt\n",
    "txt_files = [f for f in os.listdir(txt_dir) if f.lower().endswith('.txt') and \"license\" not in f.lower()]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for filename in txt_files:\n",
    "    with open(os.path.join(txt_dir, filename), encoding='utf-8') as f:\n",
    "        cat = None\n",
    "        subcat = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Top-level CATEGORY (all uppercase, no punctuation)\n",
    "            if re.fullmatch(r'[A-Z ]+', line):\n",
    "                cat = line\n",
    "                subcat = None  # Reset subcategory\n",
    "                continue\n",
    "\n",
    "            # SUBCATEGORY (all uppercase, possibly with dots, hyphens, or spaces)\n",
    "            if re.fullmatch(r'[A-Z0-9\\.\\-\\_ ]+', line) and not line.endswith(')'):\n",
    "                subcat = line\n",
    "                continue\n",
    "\n",
    "            # WORD line (all uppercase, possibly with hyphens or underscores, ends with (n))\n",
    "            m = re.match(r'^([A-Z0-9\\-_ ]+)\\s+\\(\\d+\\)$', line)\n",
    "            if m:\n",
    "                word = m.group(1).strip()\n",
    "                entries.append({\n",
    "                    \"word\": word,\n",
    "                    \"category\": cat,\n",
    "                    \"subcategory\": subcat,\n",
    "                    \"source_file\": filename\n",
    "                })\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(entries)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=[\"word\", \"category\", \"subcategory\"])\n",
    "\n",
    "# Export the main cleaned dataset\n",
    "df.to_csv(\"all_wordnet_cleaned.csv\", index=False)\n",
    "print(f\"Total entries (deduplicated): {len(df)}\")\n",
    "\n",
    "# Export a list of all unique categories and subcategories\n",
    "categories = df[['category', 'subcategory']].drop_duplicates().sort_values(['category', 'subcategory'])\n",
    "categories.to_csv(\"all_wordnet_categories.csv\", index=False)\n",
    "print(\"Categories exported to all_wordnet_categories.csv\")\n",
    "\n",
    "# Optional: Show a few rows as preview\n",
    "print(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5cedea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neelb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified keyword list saved to: C:\\Users\\neelb\\Desktop\\MEDD8925 - Analyzing Data Quantitatively\\MEDD8925 Discord Project\\classified_keywords.csv\n",
      "   lower_word    category         subcategory\n",
      "0           n  ADJECTIVES            ADJ.PERT\n",
      "1           n  ADJECTIVES             ADJ.ALL\n",
      "2           n     ADVERBS             ADV.ALL\n",
      "3           n       NOUNS           NOUN.TOPS\n",
      "4           n       NOUNS       NOUN.ARTIFACT\n",
      "5           n       NOUNS           NOUN.BODY\n",
      "6           n       NOUNS  NOUN.COMMUNICATION\n",
      "7           n       NOUNS         NOUN.OBJECT\n",
      "8           n       NOUNS         NOUN.PERSON\n",
      "9           n       NOUNS        NOUN.PROCESS\n",
      "10          n       NOUNS       NOUN.QUANTITY\n",
      "11          n       NOUNS       NOUN.RELATION\n",
      "12          n       NOUNS      NOUN.SUBSTANCE\n",
      "13          n       NOUNS           NOUN.TIME\n",
      "14          e     ADVERBS             ADV.ALL\n",
      "15          e       NOUNS  NOUN.COMMUNICATION\n",
      "16          e       NOUNS       NOUN.QUANTITY\n",
      "17          e       NOUNS       NOUN.RELATION\n",
      "18          e       NOUNS      NOUN.SUBSTANCE\n",
      "19          e       VERBS           VERB.BODY\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- File paths ---\n",
    "wordnet_path = r\"C:\\Users\\neelb\\Desktop\\MEDD8925 - Analyzing Data Quantitatively\\MEDD8925 Discord Project\\all_wordnet_cleaned.csv\"\n",
    "keywords_path = r\"C:\\Users\\neelb\\Desktop\\MEDD8925 - Analyzing Data Quantitatively\\MEDD8925 Discord Project\\keywordlist.csv\"\n",
    "output_path = r\"C:\\Users\\neelb\\Desktop\\MEDD8925 - Analyzing Data Quantitatively\\MEDD8925 Discord Project\\classified_keywords.csv\"\n",
    "\n",
    "# --- Load Data ---\n",
    "df_wordnet = pd.read_csv(wordnet_path)\n",
    "df_keywords = pd.read_csv(keywords_path)\n",
    "\n",
    "# --- Clean and Remove Stopwords ---\n",
    "keyword_col = 'word' if 'word' in df_keywords.columns else df_keywords.columns[0]\n",
    "df_keywords['lower_word'] = df_keywords[keyword_col].str.lower()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_keywords = df_keywords[~df_keywords['lower_word'].isin(stop_words)]\n",
    "\n",
    "# --- Prepare WordNet for Matching ---\n",
    "df_wordnet['lower_word'] = df_wordnet['word'].str.lower()\n",
    "\n",
    "# --- Merge/Join ---\n",
    "merged = pd.merge(df_keywords, df_wordnet, on='lower_word', how='left', suffixes=('_keyword', '_wordnet'))\n",
    "\n",
    "# --- Save Output ---\n",
    "merged.to_csv(output_path, index=False)\n",
    "print(f\"Classified keyword list saved to: {output_path}\")\n",
    "\n",
    "# Preview some results\n",
    "print(merged[['lower_word', 'category', 'subcategory']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694156b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
